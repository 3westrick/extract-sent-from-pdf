{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-community langchain-openai faiss-cpu sentence-transformers pdfplumber PyMuPDF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzyvqiNghmT_",
        "outputId": "24707f7d-49ac-424a-a5b9-415fa2c6ec8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.30-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.12.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (5.0.0)\n",
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.7-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyMuPDF\n",
            "  Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.72)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.12)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.42)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Collecting langchain-core<1.0.0,>=0.3.72 (from langchain)\n",
            "  Downloading langchain_core-0.3.74-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting openai<2.0.0,>=1.99.9 (from langchain-openai)\n",
            "  Downloading openai-1.99.9-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.55.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.16.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.34.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Collecting pdfminer.six==20250506 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n",
            "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.3.30-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.4/74.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.12.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfplumber-0.11.7-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m90.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading langchain_core-0.3.74-py3-none-any.whl (443 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.5/443.5 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.99.9-py3-none-any.whl (786 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m786.8/786.8 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, pypdfium2, PyMuPDF, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mypy-extensions, marshmallow, httpx-sse, faiss-cpu, typing-inspect, nvidia-cusparse-cu12, nvidia-cudnn-cu12, pydantic-settings, pdfminer.six, openai, nvidia-cusolver-cu12, dataclasses-json, pdfplumber, langchain-core, langchain-openai, langchain-community\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.99.1\n",
            "    Uninstalling openai-1.99.1:\n",
            "      Successfully uninstalled openai-1.99.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.72\n",
            "    Uninstalling langchain-core-0.3.72:\n",
            "      Successfully uninstalled langchain-core-0.3.72\n",
            "Successfully installed PyMuPDF-1.26.3 dataclasses-json-0.6.7 faiss-cpu-1.12.0 httpx-sse-0.4.1 langchain-community-0.3.27 langchain-core-0.3.74 langchain-openai-0.3.30 marshmallow-3.26.1 mypy-extensions-1.1.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 openai-1.99.9 pdfminer.six-20250506 pdfplumber-0.11.7 pydantic-settings-2.10.1 pypdfium2-4.30.0 python-dotenv-1.1.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download() # d -> all"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lmfMRN7okG4",
        "outputId": "3cca398d-cf39-47a4-d7c4-16f9c3d9ff18"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> all\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "    Downloading collection 'all'\n",
            "       | \n",
            "       | Downloading package abc to /root/nltk_data...\n",
            "       |   Unzipping corpora/abc.zip.\n",
            "       | Downloading package alpino to /root/nltk_data...\n",
            "       |   Unzipping corpora/alpino.zip.\n",
            "       | Downloading package averaged_perceptron_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "       | Downloading package averaged_perceptron_tagger_eng to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "       | Downloading package averaged_perceptron_tagger_ru to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger_ru.zip.\n",
            "       | Downloading package averaged_perceptron_tagger_rus to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger_rus.zip.\n",
            "       | Downloading package basque_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/basque_grammars.zip.\n",
            "       | Downloading package bcp47 to /root/nltk_data...\n",
            "       | Downloading package biocreative_ppi to /root/nltk_data...\n",
            "       |   Unzipping corpora/biocreative_ppi.zip.\n",
            "       | Downloading package bllip_wsj_no_aux to /root/nltk_data...\n",
            "       |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "       | Downloading package book_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/book_grammars.zip.\n",
            "       | Downloading package brown to /root/nltk_data...\n",
            "       |   Unzipping corpora/brown.zip.\n",
            "       | Downloading package brown_tei to /root/nltk_data...\n",
            "       |   Unzipping corpora/brown_tei.zip.\n",
            "       | Downloading package cess_cat to /root/nltk_data...\n",
            "       |   Unzipping corpora/cess_cat.zip.\n",
            "       | Downloading package cess_esp to /root/nltk_data...\n",
            "       |   Unzipping corpora/cess_esp.zip.\n",
            "       | Downloading package chat80 to /root/nltk_data...\n",
            "       |   Unzipping corpora/chat80.zip.\n",
            "       | Downloading package city_database to /root/nltk_data...\n",
            "       |   Unzipping corpora/city_database.zip.\n",
            "       | Downloading package cmudict to /root/nltk_data...\n",
            "       |   Unzipping corpora/cmudict.zip.\n",
            "       | Downloading package comparative_sentences to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping corpora/comparative_sentences.zip.\n",
            "       | Downloading package comtrans to /root/nltk_data...\n",
            "       | Downloading package conll2000 to /root/nltk_data...\n",
            "       |   Unzipping corpora/conll2000.zip.\n",
            "       | Downloading package conll2002 to /root/nltk_data...\n",
            "       |   Unzipping corpora/conll2002.zip.\n",
            "       | Downloading package conll2007 to /root/nltk_data...\n",
            "       | Downloading package crubadan to /root/nltk_data...\n",
            "       |   Unzipping corpora/crubadan.zip.\n",
            "       | Downloading package dependency_treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/dependency_treebank.zip.\n",
            "       | Downloading package dolch to /root/nltk_data...\n",
            "       |   Unzipping corpora/dolch.zip.\n",
            "       | Downloading package english_wordnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/english_wordnet.zip.\n",
            "       | Downloading package europarl_raw to /root/nltk_data...\n",
            "       |   Unzipping corpora/europarl_raw.zip.\n",
            "       | Downloading package extended_omw to /root/nltk_data...\n",
            "       | Downloading package floresta to /root/nltk_data...\n",
            "       |   Unzipping corpora/floresta.zip.\n",
            "       | Downloading package framenet_v15 to /root/nltk_data...\n",
            "       |   Unzipping corpora/framenet_v15.zip.\n",
            "       | Downloading package framenet_v17 to /root/nltk_data...\n",
            "       |   Unzipping corpora/framenet_v17.zip.\n",
            "       | Downloading package gazetteers to /root/nltk_data...\n",
            "       |   Unzipping corpora/gazetteers.zip.\n",
            "       | Downloading package genesis to /root/nltk_data...\n",
            "       |   Unzipping corpora/genesis.zip.\n",
            "       | Downloading package gutenberg to /root/nltk_data...\n",
            "       |   Unzipping corpora/gutenberg.zip.\n",
            "       | Downloading package ieer to /root/nltk_data...\n",
            "       |   Unzipping corpora/ieer.zip.\n",
            "       | Downloading package inaugural to /root/nltk_data...\n",
            "       |   Unzipping corpora/inaugural.zip.\n",
            "       | Downloading package indian to /root/nltk_data...\n",
            "       |   Unzipping corpora/indian.zip.\n",
            "       | Downloading package jeita to /root/nltk_data...\n",
            "       | Downloading package kimmo to /root/nltk_data...\n",
            "       |   Unzipping corpora/kimmo.zip.\n",
            "       | Downloading package knbc to /root/nltk_data...\n",
            "       | Downloading package large_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/large_grammars.zip.\n",
            "       | Downloading package lin_thesaurus to /root/nltk_data...\n",
            "       |   Unzipping corpora/lin_thesaurus.zip.\n",
            "       | Downloading package mac_morpho to /root/nltk_data...\n",
            "       |   Unzipping corpora/mac_morpho.zip.\n",
            "       | Downloading package machado to /root/nltk_data...\n",
            "       | Downloading package masc_tagged to /root/nltk_data...\n",
            "       | Downloading package maxent_ne_chunker to /root/nltk_data...\n",
            "       |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "       | Downloading package maxent_ne_chunker_tab to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "       | Downloading package maxent_treebank_pos_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "       | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/maxent_treebank_pos_tagger_tab.zip.\n",
            "       | Downloading package mock_corpus to /root/nltk_data...\n",
            "       | Downloading package moses_sample to /root/nltk_data...\n",
            "       |   Unzipping models/moses_sample.zip.\n",
            "       | Downloading package movie_reviews to /root/nltk_data...\n",
            "       |   Unzipping corpora/movie_reviews.zip.\n",
            "       | Downloading package mte_teip5 to /root/nltk_data...\n",
            "       |   Unzipping corpora/mte_teip5.zip.\n",
            "       | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "       |   Unzipping misc/mwa_ppdb.zip.\n",
            "       | Downloading package names to /root/nltk_data...\n",
            "       |   Unzipping corpora/names.zip.\n",
            "       | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "       | Downloading package nonbreaking_prefixes to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "       | Downloading package nps_chat to /root/nltk_data...\n",
            "       |   Unzipping corpora/nps_chat.zip.\n",
            "       | Downloading package omw to /root/nltk_data...\n",
            "       | Downloading package omw-1.4 to /root/nltk_data...\n",
            "       | Downloading package opinion_lexicon to /root/nltk_data...\n",
            "       |   Unzipping corpora/opinion_lexicon.zip.\n",
            "       | Downloading package panlex_swadesh to /root/nltk_data...\n",
            "       | Downloading package paradigms to /root/nltk_data...\n",
            "       |   Unzipping corpora/paradigms.zip.\n",
            "       | Downloading package pe08 to /root/nltk_data...\n",
            "       |   Unzipping corpora/pe08.zip.\n",
            "       | Downloading package perluniprops to /root/nltk_data...\n",
            "       |   Unzipping misc/perluniprops.zip.\n",
            "       | Downloading package pil to /root/nltk_data...\n",
            "       |   Unzipping corpora/pil.zip.\n",
            "       | Downloading package pl196x to /root/nltk_data...\n",
            "       |   Unzipping corpora/pl196x.zip.\n",
            "       | Downloading package porter_test to /root/nltk_data...\n",
            "       |   Unzipping stemmers/porter_test.zip.\n",
            "       | Downloading package ppattach to /root/nltk_data...\n",
            "       |   Unzipping corpora/ppattach.zip.\n",
            "       | Downloading package problem_reports to /root/nltk_data...\n",
            "       |   Unzipping corpora/problem_reports.zip.\n",
            "       | Downloading package product_reviews_1 to /root/nltk_data...\n",
            "       |   Unzipping corpora/product_reviews_1.zip.\n",
            "       | Downloading package product_reviews_2 to /root/nltk_data...\n",
            "       |   Unzipping corpora/product_reviews_2.zip.\n",
            "       | Downloading package propbank to /root/nltk_data...\n",
            "       | Downloading package pros_cons to /root/nltk_data...\n",
            "       |   Unzipping corpora/pros_cons.zip.\n",
            "       | Downloading package ptb to /root/nltk_data...\n",
            "       |   Unzipping corpora/ptb.zip.\n",
            "       | Downloading package punkt to /root/nltk_data...\n",
            "       |   Unzipping tokenizers/punkt.zip.\n",
            "       | Downloading package punkt_tab to /root/nltk_data...\n",
            "       |   Unzipping tokenizers/punkt_tab.zip.\n",
            "       | Downloading package qc to /root/nltk_data...\n",
            "       |   Unzipping corpora/qc.zip.\n",
            "       | Downloading package reuters to /root/nltk_data...\n",
            "       | Downloading package rslp to /root/nltk_data...\n",
            "       |   Unzipping stemmers/rslp.zip.\n",
            "       | Downloading package rte to /root/nltk_data...\n",
            "       |   Unzipping corpora/rte.zip.\n",
            "       | Downloading package sample_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/sample_grammars.zip.\n",
            "       | Downloading package semcor to /root/nltk_data...\n",
            "       | Downloading package senseval to /root/nltk_data...\n",
            "       |   Unzipping corpora/senseval.zip.\n",
            "       | Downloading package sentence_polarity to /root/nltk_data...\n",
            "       |   Unzipping corpora/sentence_polarity.zip.\n",
            "       | Downloading package sentiwordnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/sentiwordnet.zip.\n",
            "       | Downloading package shakespeare to /root/nltk_data...\n",
            "       |   Unzipping corpora/shakespeare.zip.\n",
            "       | Downloading package sinica_treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/sinica_treebank.zip.\n",
            "       | Downloading package smultron to /root/nltk_data...\n",
            "       |   Unzipping corpora/smultron.zip.\n",
            "       | Downloading package snowball_data to /root/nltk_data...\n",
            "       | Downloading package spanish_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/spanish_grammars.zip.\n",
            "       | Downloading package state_union to /root/nltk_data...\n",
            "       |   Unzipping corpora/state_union.zip.\n",
            "       | Downloading package stopwords to /root/nltk_data...\n",
            "       |   Unzipping corpora/stopwords.zip.\n",
            "       | Downloading package subjectivity to /root/nltk_data...\n",
            "       |   Unzipping corpora/subjectivity.zip.\n",
            "       | Downloading package swadesh to /root/nltk_data...\n",
            "       |   Unzipping corpora/swadesh.zip.\n",
            "       | Downloading package switchboard to /root/nltk_data...\n",
            "       |   Unzipping corpora/switchboard.zip.\n",
            "       | Downloading package tagsets to /root/nltk_data...\n",
            "       |   Unzipping help/tagsets.zip.\n",
            "       | Downloading package tagsets_json to /root/nltk_data...\n",
            "       |   Unzipping help/tagsets_json.zip.\n",
            "       | Downloading package timit to /root/nltk_data...\n",
            "       |   Unzipping corpora/timit.zip.\n",
            "       | Downloading package toolbox to /root/nltk_data...\n",
            "       |   Unzipping corpora/toolbox.zip.\n",
            "       | Downloading package treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/treebank.zip.\n",
            "       | Downloading package twitter_samples to /root/nltk_data...\n",
            "       |   Unzipping corpora/twitter_samples.zip.\n",
            "       | Downloading package udhr to /root/nltk_data...\n",
            "       |   Unzipping corpora/udhr.zip.\n",
            "       | Downloading package udhr2 to /root/nltk_data...\n",
            "       |   Unzipping corpora/udhr2.zip.\n",
            "       | Downloading package unicode_samples to /root/nltk_data...\n",
            "       |   Unzipping corpora/unicode_samples.zip.\n",
            "       | Downloading package universal_tagset to /root/nltk_data...\n",
            "       |   Unzipping taggers/universal_tagset.zip.\n",
            "       | Downloading package universal_treebanks_v20 to\n",
            "       |     /root/nltk_data...\n",
            "       | Downloading package vader_lexicon to /root/nltk_data...\n",
            "       | Downloading package verbnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/verbnet.zip.\n",
            "       | Downloading package verbnet3 to /root/nltk_data...\n",
            "       |   Unzipping corpora/verbnet3.zip.\n",
            "       | Downloading package webtext to /root/nltk_data...\n",
            "       |   Unzipping corpora/webtext.zip.\n",
            "       | Downloading package wmt15_eval to /root/nltk_data...\n",
            "       |   Unzipping models/wmt15_eval.zip.\n",
            "       | Downloading package word2vec_sample to /root/nltk_data...\n",
            "       |   Unzipping models/word2vec_sample.zip.\n",
            "       | Downloading package wordnet to /root/nltk_data...\n",
            "       | Downloading package wordnet2021 to /root/nltk_data...\n",
            "       | Downloading package wordnet2022 to /root/nltk_data...\n",
            "       |   Unzipping corpora/wordnet2022.zip.\n",
            "       | Downloading package wordnet31 to /root/nltk_data...\n",
            "       | Downloading package wordnet_ic to /root/nltk_data...\n",
            "       |   Unzipping corpora/wordnet_ic.zip.\n",
            "       | Downloading package words to /root/nltk_data...\n",
            "       |   Unzipping corpora/words.zip.\n",
            "       | Downloading package ycoe to /root/nltk_data...\n",
            "       |   Unzipping corpora/ycoe.zip.\n",
            "       | \n",
            "     Done downloading collection all\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUbpsdA7g5zB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import fitz  # PyMuPDF is still used for creating the dummy PDF\n",
        "from langchain_community.document_loaders import PyPDFLoader, PDFPlumberLoader, PDFPlumberLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, TextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "from langchain.docstore.document import Document # To create new Document objects\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "# from langchain_openai import ChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_pdf_path = \"AI_powered_integration_of_multi_source_data_for_TAA_discovery_to.pdf\"\n",
        "# doc = fitz.open()\n",
        "# page1 = doc.new_page()\n",
        "# page1.insert_text((72, 72), \"The Solar System: A Brief Overview\")\n",
        "# page1.insert_text((72, 108), \"The solar system consists of the Sun and everything that orbits it. This includes eight planets and their moons, dwarf planets, asteroids, and comets.\")\n",
        "# page1.insert_text((72, 144), \"Mars, known as the Red Planet, is the fourth planet from the Sun. It has a thin atmosphere and two small moons, Phobos and Deimos.\")\n",
        "\n",
        "# page2 = doc.new_page()\n",
        "# page2.insert_text((72, 72), \"Jupiter and its Moons\")\n",
        "# page2.insert_text((72, 108), \"Jupiter is the largest planet in our solar system. It is a gas giant, primarily composed of hydrogen and helium.\")\n",
        "# page2.insert_text((72, 144), \"It has a famous Great Red Spot, which is a giant storm raging for centuries. Jupiter has dozens of moons, with the four largest being the Galilean moons: Io, Europa, Ganymede, and Callisto.\")\n",
        "\n",
        "# doc.save(dummy_pdf_path)\n",
        "# doc.close()\n",
        "# print(f\"INFO: Created a dummy PDF for demonstration: '{dummy_pdf_path}'\")"
      ],
      "metadata": {
        "id": "xde-zRqajHoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pdf_documents(file_path):\n",
        "  document_loader = PDFPlumberLoader(file_path)\n",
        "  return document_loader.load()"
      ],
      "metadata": {
        "id": "ratCkwUhjLbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_docs = load_pdf_documents(dummy_pdf_path)"
      ],
      "metadata": {
        "id": "1Ui86Akdk5R8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_docs[0].metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znL6JY19lCJd",
        "outputId": "f501bfd2-15e9-45a1-89ad-b2cb3365dead"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'source': 'AI_powered_integration_of_multi_source_data_for_TAA_discovery_to.pdf',\n",
              " 'file_path': 'AI_powered_integration_of_multi_source_data_for_TAA_discovery_to.pdf',\n",
              " 'page': 0,\n",
              " 'total_pages': 9,\n",
              " 'Author': 'Xie, Tao',\n",
              " 'Comments': '',\n",
              " 'Company': '',\n",
              " 'CreationDate': \"D:20250506175120-07'00'\",\n",
              " 'Creator': 'Acrobat PDFMaker 11 for Word',\n",
              " 'Keywords': '',\n",
              " 'MSIP_Label_4791b42f-c435-42ca-9531-75a3f42aae3d_ActionId': '296b9d1c-3a3b-4187-9aa1-0048fda51a4c',\n",
              " 'MSIP_Label_4791b42f-c435-42ca-9531-75a3f42aae3d_ContentBits': '0',\n",
              " 'MSIP_Label_4791b42f-c435-42ca-9531-75a3f42aae3d_Enabled': 'true',\n",
              " 'MSIP_Label_4791b42f-c435-42ca-9531-75a3f42aae3d_Method': 'Privileged',\n",
              " 'MSIP_Label_4791b42f-c435-42ca-9531-75a3f42aae3d_Name': '4791b42f-c435-42ca-9531-75a3f42aae3d',\n",
              " 'MSIP_Label_4791b42f-c435-42ca-9531-75a3f42aae3d_SetDate': '2025-03-20T21:56:47Z',\n",
              " 'MSIP_Label_4791b42f-c435-42ca-9531-75a3f42aae3d_SiteId': '7a916015-20ae-4ad1-9170-eefd915e9272',\n",
              " 'MSIP_Label_4791b42f-c435-42ca-9531-75a3f42aae3d_Tag': '10, 0, 1, 1',\n",
              " 'ModDate': \"D:20250809125613-07'00'\",\n",
              " 'Producer': 'Adobe PDF Library 11.0',\n",
              " 'SourceModified': 'D:20250507004936',\n",
              " 'Subject': '',\n",
              " 'Title': ''}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_docs[0].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "XsqDNm-ileC8",
        "outputId": "6104b5df-0f18-4908-d480-7749d2be1b2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'bioRxiv preprint doi: https://doi.org/10.1101/2025.05.06.652559; this version posted May 8, 2025. The copyright holder for this preprint (which\\nwas not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made\\navailable under aCC-BY-NC-ND 4.0 International license.\\nAI-powered integration of multi-source data for TAA discovery to\\naccelerate ADC and TCE drug development (I):\\nTAA Target Identification and Prioritization\\nTao Xie* & Chao-Hui Huang*\\n*: co-first authors\\nEmail: Tao.Xie@pfizer.com\\nChao-Hui.Huang@pfizer.com\\nAbstract:\\nThe advancement of T-cell engagers (TCEs) and antibody-drug conjugates (ADCs) has been\\nhindered by fragmented data landscapes. This paper, the first in a series, introduces an AI-driven\\nframework specifically for tumor-associated antigen (TAA) target identification and prioritization,\\na critical initial step in TCE and ADC development. Our framework integrates diverse datasets—\\nincluding multi-omics repositories and information from scientific publications—to systematically\\nenhance the discovery of TAAs. We have developed a graph retrieval-augmented generation\\n(RAG)-enhanced language model that extracts insights from biological and clinical literature,\\nwhile integrating curated public oncology-related omics databases such as TCGA, GTEx, single-\\ncell atlases, and additional omics datasets. This approach prioritizes TAAs with high tumor\\nselectivity and low on-target/off-tumor risk. By unifying diverse knowledge sources, our method\\nprovides a scalable, efficient, and data-agnostic strategy to address attrition challenges in both\\nADC and TCE drug development pipelines, focusing initially on TAA target identification and\\nprioritization to transform the landscape of cancer therapeutics.\\nKeywords: Tumor-associated antigens, antibody-drug conjugates, multi-omics data analysis, T-\\ncell engagers, large language models, GraphRAG.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"INFO: Splitting document into sentences...\")\n",
        "sentence_docs = []\n",
        "for doc in raw_docs:\n",
        "    # Extract content and metadata from the original document (page)\n",
        "    content = doc.page_content\n",
        "    metadata = doc.metadata.copy() # Make a copy to avoid modifying the original\n",
        "\n",
        "\n",
        "\n",
        "    # sentences = content.split(\".\")\n",
        "\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', content.strip())\n",
        "\n",
        "    # Filter out empty strings\n",
        "    sentences = [s for s in sentences if s.strip() and len(s) > 3 and \" \" in s]\n",
        "\n",
        "    # Create a new Document for each sentence with the same metadata\n",
        "    for sentence in sentences:\n",
        "        # new_sent = sentence.replace(\"\\n\", \" \")\n",
        "        # if new_sent.strip() == \"\":\n",
        "        #     continue\n",
        "        new_doc = Document(page_content=sentence.replace(\"\\n\", \" \"), metadata=metadata)\n",
        "        sentence_docs.append(new_doc)\n",
        "\n",
        "print(f\"INFO: Created {len(sentence_docs)} sentence-level documents.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UN6ngCtCmKCN",
        "outputId": "627040da-8b65-4916-bd4a-f73d5a6330e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Splitting document into sentences...\n",
            "INFO: Created 141 sentence-level documents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sent in sentence_docs:\n",
        "  print(sent.page_content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIDvXiVI0apH",
        "outputId": "cf60dd45-eb48-4de0-f761-82a26f8297b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bioRxiv preprint doi: https://doi.org/10.1101/2025.05.06.652559; this version posted May 8, 2025.\n",
            "The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity.\n",
            "It is made available under aCC-BY-NC-ND 4.0 International license.\n",
            "AI-powered integration of multi-source data for TAA discovery to accelerate ADC and TCE drug development (I): TAA Target Identification and Prioritization Tao Xie* & Chao-Hui Huang* *: co-first authors Email: Tao.Xie@pfizer.com Chao-Hui.Huang@pfizer.com Abstract: The advancement of T-cell engagers (TCEs) and antibody-drug conjugates (ADCs) has been hindered by fragmented data landscapes.\n",
            "This paper, the first in a series, introduces an AI-driven framework specifically for tumor-associated antigen (TAA) target identification and prioritization, a critical initial step in TCE and ADC development.\n",
            "Our framework integrates diverse datasets— including multi-omics repositories and information from scientific publications—to systematically enhance the discovery of TAAs.\n",
            "We have developed a graph retrieval-augmented generation (RAG)-enhanced language model that extracts insights from biological and clinical literature, while integrating curated public oncology-related omics databases such as TCGA, GTEx, single- cell atlases, and additional omics datasets.\n",
            "This approach prioritizes TAAs with high tumor selectivity and low on-target/off-tumor risk.\n",
            "By unifying diverse knowledge sources, our method provides a scalable, efficient, and data-agnostic strategy to address attrition challenges in both ADC and TCE drug development pipelines, focusing initially on TAA target identification and prioritization to transform the landscape of cancer therapeutics.\n",
            "Keywords: Tumor-associated antigens, antibody-drug conjugates, multi-omics data analysis, T- cell engagers, large language models, GraphRAG.\n",
            "bioRxiv preprint doi: https://doi.org/10.1101/2025.05.06.652559; this version posted May 8, 2025.\n",
            "The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity.\n",
            "It is made available under aCC-BY-NC-ND 4.0 International license.\n",
            "Introduction: Cancer remains a significant global health challenge, emphasizing the need for innovative therapeutic strategies.\n",
            "Tumor-associated antigens (TAAs) play a crucial role in cancer treatment by serving as precise targets for Antibody-drug conjugates (ADCs) and T-cell engagers (TCEs) [1].\n",
            "These platforms are designed to eradicate cancer cells while minimizing harm to healthy tissues.\n",
            "TCEs function by redirecting T-cells to attack cancer cells, utilizing the body's immune system to effectively eliminate malignant cells while ADCs leverage the specificity of antibodies to deliver potent cytotoxic drugs directly to cancer cells, ensuring targeted treatment.\n",
            "Despite advancements in genomics and bioinformatics that have deepened our understanding of tumor biology, converting this knowledge into practical therapeutic TAA targets remains challenging.\n",
            "The success of TCEs and ADCs heavily depends on rigorous target selection.\n",
            "Identifying and prioritizing suitable TAAs is vital for enhancing therapeutic precision and reducing off-target effects.\n",
            "Without effective target triage, the ability of TCEs and ADCs to deliver focused and effective cancer treatment is significantly undermined [1].\n",
            "The integration of machine learning with biological expertise, particularly through large language models (LLMs), holds promise for revolutionizing the analysis of complex biological data.\n",
            "In the realm of oncology, LLMs show vast potential to reshape data analysis by facilitating rapid interpretations and contributing to the development of personal treatment plans tailored to individual patient needs [2-4].\n",
            "Domain-specific LLMs, aided by specific Retrieval Augmented Generation (RAG), excel in tasks defined by organizational standards.\n",
            "These models are tailored for real-world applications, requiring a deep understanding of context, product data, corporate policies, and industry terminology.\n",
            "Given the advent of LLMs promises a transformative approach, offering unprecedented opportunities in, this study demonstrates the application of a graph RAG- enhanced LLM (foundation model: ChatGPT-4) to drive the discovery of TAAs, thus enhancing target selectivity and therapeutic precision for ADC and TCE drug development.\n",
            "bioRxiv preprint doi: https://doi.org/10.1101/2025.05.06.652559; this version posted May 8, 2025.\n",
            "The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity.\n",
            "It is made available under aCC-BY-NC-ND 4.0 International license.\n",
            "Materials and Methods Knowledge Data from PubMed Our study builds upon existing research in graph retrieval-augmented generation (RAG) and seeks to extend its application within the domain of tumor-associated antigen (TAA) discovery.\n",
            "Utilizing the E-utilities suite of NCBI Web APIs, we accessed the most recent open-access full-text articles focused on TAA research in lung cancer.\n",
            "We crafted targeted queries using keywords such as \"tumor-associated antigen\" and \"lung cancer\" to capture a comprehensive array of relevant literature.\n",
            "Python-based API calls were developed to retrieve PubMed IDs (PMIDs) corresponding to these queries via the esearch function, interfacing with the Entrez system to locate pertinent articles.\n",
            "Subsequently, the efetch function was employed to obtain detailed metadata for each PMID, including PubMed Central IDs (PMCIDs), article titles, abstracts, and more.\n",
            "To maintain stability and adhere to Entrez API rate limits, batch processing with pauses was implemented.\n",
            "All retrieved records were stored in txt format, facilitating seamless integration with data analysis tools.\n",
            "Oncology Omics Data Process The framework is designed to integrate diverse multi-omics data types, encompassing bulk RNA expression, single-cell omics, spatial transcriptomics, and protein expression data.\n",
            "This comprehensive approach significantly enhances the discovery and prioritization of tumor- associated antigens (TAAs).\n",
            "To illustrate the integration of omics data, we present an overview of the process for handling bulk gene expression data from the TCGA and GTEx datasets.\n",
            "Building on the findings of a previous study [4], we utilized bulk expression data from the UCSC RNA-seq Compendium.\n",
            "This resource standardizes TCGA and GTEx samples from both cancerous and normal tissues by employing advanced computational techniques to eliminate batch effects.\n",
            "The integration of these datasets enables a robust analysis of gene expression differences between tumor and normal tissues, thereby facilitating the identification of potential TAAs.\n",
            "To augment the safety assessment of TAA candidates, a safety score—developed to evaluate the expression of TAAs in normal tissues relative to tumor tissues—has been incorporated into the RAG framework.\n",
            "This score is derived from the integrated expression data, as outlined below:\n",
            "bioRxiv preprint doi: https://doi.org/10.1101/2025.05.06.652559; this version posted May 8, 2025.\n",
            "The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity.\n",
            "It is made available under aCC-BY-NC-ND 4.0 International license.\n",
            "Z-score Calculation: For each gene, z-scores are calculated based on its expression level across different tissue types.\n",
            "This involves normalizing the expression data by subtracting the mean expression level and dividing by the standard deviation, thereby highlighting deviations from average expression.\n",
            "Assessment of Normal Tissue Expression: The z-scores provide insights into how significantly a gene's expression deviates in normal tissues compared to tumor tissues.\n",
            "Genes with high z-scores (>1) in normal tissues indicate higher expression levels relative to the average, which could suggest potential safety concerns if those genes are targeted in cancer therapy.\n",
            "Safety Score Integration: The final score integrates these z-scores to quantify the risk associated with targeting a particular gene as a TAA.\n",
            "note that normal prostate tissue was excluded from the normal tissue collection, as it is not deemed critical for this analysis.\n",
            "With these steps, we have generated safety scores considering the expression levels across various normal tissues to ensure that selected TAAs have minimal expression in non-cancerous tissues, thus reducing the likelihood of off-target effects.\n",
            "The information can be found in the supplementary (Supplementary Table 1).\n",
            "Construction of Biological Knowledge GraphRAG In this study, we chose OpenAI's GPT-4 for its real-time responsiveness and extensive context- handling abilities.\n",
            "While versatile, GPT-4 needs enhancements for specialized tasks like TAA discovery.\n",
            "The Graph RAG approach [6] employs cutting-edge retrieval algorithms to gather relevant biological data, which GPT-4 then synthesizes.\n",
            "Leveraging advancements in NLP and data retrieval, the system generates detailed insights into TAA gene-cancer type connections, enhancing interpretative capabilities and information accuracy.\n",
            "Our RAG framework is refined through continuous updates with new findings and iterative graph structure modifications, ensuring accurate TAA identification and characterization.\n",
            "This dynamic approach allows seamless integration of new data, keeping the model at the forefront of TAA\n",
            "bioRxiv preprint doi: https://doi.org/10.1101/2025.05.06.652559; this version posted May 8, 2025.\n",
            "The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity.\n",
            "It is made available under aCC-BY-NC-ND 4.0 International license.\n",
            "A reinforcement component within the RAG framework implements a feedback loop, critically evaluating outputs to further refine the model.\n",
            "This continuous learning mechanism is crucial for maintaining performance and relevance in the evolving cancer research landscape.\n",
            "We carefully integrate prompt templates, queries, and gene-aware contexts to construct comprehensive prompts.\n",
            "These prompts are submitted to GPT-4 with key parameters configured to facilitate the generation of nuanced completions.\n",
            "The Graph RAG-enhanced LLM output includes assessments of TAA gene-cancer type relationships and human-readable explanations, as well as safety assessment based on Omics data analysis, aiding scientific understanding and supporting therapeutic development with domain specific knowledge graph (e.g.\n",
            "By refining and expanding the framework, we ensure the model remains a valuable tool for advancing cancer research and therapy innovation Figure 1: A subset of knowledge graph extracted from the given PubMed articles.\n",
            "Knowledge graph of entity nodes and relationship edges derived from the PubMed dataset (described in the txt), with different colors representing various communities.\n",
            "bioRxiv preprint doi: https://doi.org/10.1101/2025.05.06.652559; this version posted May 8, 2025.\n",
            "The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity.\n",
            "It is made available under aCC-BY-NC-ND 4.0 International license.\n",
            "Results: An online service platform has been developed, allowing users to input prompts that include gene symbols and a cancer type to efficiently obtain automated results.\n",
            "These results indicate the significance of the gene(s) in the specified cancer type, accompanied by human-readable explanations and a summary of data-driven learning outcomes, integrated with omics data derived safety considerations.\n",
            "We assess the framework's performance using known TAAs from clinical trials and compare the responses generated by the GraphRAG-enhanced LLM with the foundational model.\n",
            "Here is an example comparing the responses to the query: \"Evaluate TACSTD2 and MUC1 as Tumor-Associated Antigens in NSCLC; which one is a better ADC/TCE target?\" between the outputs of the base model and the RAG-enhanced model: Table 1 Model Response Comparison Aspect Foundation LLM Response RAG-Enhanced LLM Response TACSTD2, also known as TROP2, is a cell surface glycoprotein that has been studied primarily in breast cancer, where it is associated with epithelial subtypes and TACSTD2 is expressed in various cancers, including correlates with the expression of E-Cadherin, an epithelial TACSTD2 lung cancer.\n",
            "It plays a role in cell adhesion, but its marker [Data: Entities (6542); Relationships (8653)].\n",
            "pathogenic role is not fully understood.\n",
            "While TACSTD2 has been extensively researched in breast cancer, there is limited direct evidence regarding its role as a tumor-associated antigen (TAA) specifically in non-small cell lung cancer (NSCLC).\n",
            "MUC1 is a well-characterized tumor antigen that plays a significant role in immune responses, particularly in the context of prostate cancer treatment.\n",
            "It serves as a target MUC1 is a tumor-associated antigen that is for tumor-associated antigen-specific immune responses, overexpressed and abnormally glycosylated in many with varying response rates depending on treatment MUC1 cancers, including breast, ovarian, lung, and scheduling [Data: Entities (6409); Reports (422)].\n",
            "In pancreatic cancers.\n",
            "NSCLC, MUC1 is overexpressed and contributes to tumorigenic characteristics and metastatic propensity, making it a significant marker for poor prognosis [Data: Relationships (388, 379)].\n",
            "bioRxiv preprint doi: https://doi.org/10.1101/2025.05.06.652559; this version posted May 8, 2025.\n",
            "The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity.\n",
            "It is made available under aCC-BY-NC-ND 4.0 International license.\n",
            "TCGA/GTEx Safety score for TACSTD2 is 17 (with safety concerns) expression Not available Safety score for MUC1 is 24 (safe), expression detected in evaluation Salivary_Gland:Skin:Thyroid:Vagina Both MUC1 and TACSTD2 (EpCAM) are considered Based on the available data, MUC1 appears to be a more tumor-associated antigens (TAAs) in lung cancer.\n",
            "extensively studied and potentially promising candidate MUC1 is primarily involved in immune evasion, for a tumor-associated antigen in NSCLC.\n",
            "Its role in Conclusion whereas TACSTD2 is associated with cell adhesion, cancer progression and the development of targeted proliferation, and metastasis.\n",
            "The effectiveness of therapies, such as monoclonal antibodies and CAR T-cell each depends on the specific characteristics of the therapies, highlight its potential as a therapeutic target tumor and the therapeutic goals.\n",
            "[Data:Source (3)].\n",
            "Based on responses from both models, we found that GraphRAG enhances LLM outputs with more relevant analysis for TAA candidates and uses engineered prompts to organize the output effectively.\n",
            "The conclusion that MUC1 is a better TAA for lung cancer aligns with current clinical observations, including its use in ADC and TCE therapies [7].\n",
            "The foundation model is less specific and even wrongly connected TACSSTD2 (Trop2) to another common cancer marker EpCAM.\n",
            "The enhanced LLM identified promising TAAs, particularly in response to ADC/TCE therapeutics that induce immune responses and alter cellular protein landscapes.\n",
            "Key candidates such as MUC1 demonstrated these antigens represent viable targets for ADC and TCE therapeutic development.\n",
            "Additionally, the integrated analysis has allowed detailed safety profiling of gene expression, providing deeper insights into contextual dynamics of antigen presentation in normal tissues.\n",
            "This enhances understanding beyond traditional methodologies, highlighting the potential for integrating LLMs with domain knowledge to guide targeted therapeutic approaches.\n",
            "Conclusion and Discussion: In cancer research, large language models are being used to analyze data and interact with patients, offering new perspectives on personalized treatment plans [4].\n",
            "Our RAG-enhanced LLM offers a novel approach to TAA-based cancer therapy development, promising accelerated discovery processes and streamlined development pathways.\n",
            "Integration of LLMs with biological domain knowledge offers revolutionary potential in target discovery and prioritization, enhancing precision in cancer therapy development.\n",
            "Besides the TCGA/GTEx bulk RNAseq data for safety\n",
            "bioRxiv preprint doi: https://doi.org/10.1101/2025.05.06.652559; this version posted May 8, 2025.\n",
            "The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity.\n",
            "It is made available under aCC-BY-NC-ND 4.0 International license.\n",
            "consideration, scRNAseq and proteomics data from cancer or normal samples can also be integrated.\n",
            "Our findings underline the importance of LLM’s potential and herald a new wave of effective, selective therapeutic strategies.\n",
            "However, despite the promising potential, it is crucial to undertake rigorous validation of the identified TAAs through in vivo studies.\n",
            "Additionally, exploring combinatorial drug approaches and continuously refining our LLM-enhanced platform could significantly boost predictive power and applicability across various cancer types, thereby guiding future research and applications.\n",
            "Enhanced collaboration among AI researchers and oncologists will be vital in harnessing the full potential of LLMs to transform cancer treatment landscapes.\n",
            "The source code for generating the TAA knowledge GraphRAG will be made available on GitHub.\n",
            "In summary, this study presents a GraphRAG-enhanced model designed to identify and prioritize tumor-associated antigens (TAAs) for targeted cancer therapies.\n",
            "Using advanced NLP technologies, we demonstrated the potential of the enhanced LLM for TAA discovery and expedite therapeutic development for ADC and TCE therapies.\n",
            "Besides integrating omics features and rationales for creating detailed and interpretable analytical reports, it can be further extended to more biological datatypes such as image data from digital pathology [8].\n",
            "The integration of the powerful LLM with the multimodal omics data-derived features presents a promising approach for improving the identification of cancer genes and elucidating their roles in cancer development and progression.\n",
            "Acknowledgments: We acknowledge the guidance of the development of the project by Jadwiga Bienkowska and s contributions of our collaborators and data providers.\n",
            "bioRxiv preprint doi: https://doi.org/10.1101/2025.05.06.652559; this version posted May 8, 2025.\n",
            "The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity.\n",
            "It is made available under aCC-BY-NC-ND 4.0 International license.\n",
            "References: 1: Zhang J, et al.\n",
            "(2022) Tumor-associated antigens and their autoantibodies, from discovering to clinical utilization.\n",
            "12:970623 2: Pun F, Ozerov I, Zhavoronkov A (2023) AI-powered therapeutic target discovery, Trends Pharmacol.\n",
            "Sci., 44(9) 561-572.\n",
            "3: Quidwai M and Lagana A.\n",
            "(2024) A RAG Chatbot for Precision Medicine of Multiple Myeloma, MedRxiv, https://doi.org/10.1101/2024.03.14.24304293 4: Andrew A and Tizzard E (2024) Large language models for improving cancer diagnosis and management in primary health care settings.\n",
            "Journal of Medicine, Surgery, and Public Health.\n",
            "4:100157 5: Hu Z et al.\n",
            "(2021) The Cancer Surfaceome Atlas integrates genomic, functional and drug response data to identify actionable targets - PubMed Nat Cancer.\n",
            "2(12):1406-1422 6: Edge D et al.\n",
            "(2024) From Local to Global: A GraphRAG Approach to Query-Focused Summarization.\n",
            "ArXiv, https://arxiv.org/pdf/2404.16130.\n",
            "7: Tong X.\n",
            "et al.\n",
            "(2024) Mucin1 as a potential molecule for cancer immunotherapy and targeted therapy.\n",
            "J Cancer.\n",
            "15(1):54–67 8: Huang C.\n",
            "et al.\n",
            "(2025) Integrative whole slide image and spatial transcriptomics analysis with QuST and QuPath.\n",
            "NPJ Precision Oncolog.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFObAHhoo6el",
        "outputId": "bafba12c-b85e-472f-8e1e-bea89985f8f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/232.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "# Load the PDF\n",
        "pdf_path = \"AI_powered_integration_of_multi_source_data_for_TAA_discovery_to.pdf\"\n",
        "reader = PdfReader(pdf_path)\n",
        "\n",
        "# Extract text from all pages\n",
        "full_text = \"\"\n",
        "for page in reader.pages:\n",
        "    full_text += page.extract_text() + \"\\n\"\n",
        "\n",
        "# Split into sentences (basic approach using regex)\n",
        "# This will split on ., ?, or ! followed by a space or end of text\n",
        "sentences = re.split(r'(?<=[.!?])\\s+', full_text.strip())\n",
        "\n",
        "# Filter out empty strings\n",
        "sentences = [s for s in sentences if s.strip()]\n",
        "\n",
        "len(sentences)\n"
      ],
      "metadata": {
        "id": "eLOA6glfpJvW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7150d836-9890-4ad9-8bd4-2d678597be67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "147"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lzeq1TSz0BVD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}